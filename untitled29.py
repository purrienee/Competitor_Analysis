# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rlhlQwUTUpFnDETA89xEyPHWzjYVBScb
"""

!pip install requests beautifulsoup4 pandas matplotlib

pip install requests beautifulsoup4 pandas nltk

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import random
import time

# Download required NLTK data (only needs to be done once)
try:
    nltk.download('vader_lexicon')
except LookupError:
    print("Vader Lexicon is already downloaded")

def get_book_data(url):
    """
    Scrapes book data from a single page of Books to Scrape.

    Args:
        url (str): The URL of the Books to Scrape page.

    Returns:
        list: A list of dictionaries, where each dictionary represents a book.  Returns an empty list on error.
    """
    book_data = []
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    }
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)
        soup = BeautifulSoup(response.content, 'html.parser')

        articles = soup.find_all('article', class_='product_pod')  # Each book is in a product_pod

        for article in articles:
            title = article.h3.a['title']
            price_text = article.find('p', class_='price_color').text
            price = float(price_text[1:])  # Remove currency symbol (Â£) and convert to float

            # Extract star rating
            star_rating_element = article.find('p', class_='star-rating')
            star_rating = star_rating_element['class'][1]  # Second class is the rating (e.g., "Three")

            # Extract availability
            availability_text = article.find('p', class_='instock availability').text.strip()
            in_stock = "In stock" in availability_text

            book_data.append({
                'title': title,
                'price': price,
                'star_rating': star_rating,
                'in_stock': in_stock
            })

        return book_data

    except requests.exceptions.RequestException as e:
        print(f"Request error: {e}")
        return []  # Return empty list to indicate failure.
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return []  #Return empty list to indicate failure

def scrape_all_books(base_url, num_pages=1):
    """
    Scrapes book data from multiple pages of Books to Scrape.

    Args:
        base_url (str): The base URL of the Books to Scrape category.
        num_pages (int): The number of pages to scrape.

    Returns:
        pandas.DataFrame: A DataFrame containing the scraped book data.
    """
    all_book_data = []
    for page_num in range(1, num_pages + 1):
        url = f"{base_url}page-{page_num}.html" if page_num > 1 else base_url  # Correctly handles the first page URL
        print(f"Scraping page: {url}")
        page_data = get_book_data(url)
        if page_data:
            all_book_data.extend(page_data)
            time.sleep(random.randint(1,3)) #be nice with requests
        else:
            print(f"Failed to scrape page {page_num}. Skipping to next page.") #Non-fatal error.

    return pd.DataFrame(all_book_data) #Make DF even if some pages fail.

def analyze_pricing(df):
    """Analyzes pricing data."""
    print("\nPricing Analysis:")
    print(f"Average Price: ${df['price'].mean():.2f}")
    print(f"Median Price: ${df['price'].median():.2f}")
    print(f"Price Range: ${df['price'].min():.2f} - ${df['price'].max():.2f}")

def analyze_ratings(df):
   """Analyzes rating data."""
   print("\nRating Analysis:")
   rating_counts = df['star_rating'].value_counts().sort_index()
   print(rating_counts)

def generate_simulated_reviews(df, num_reviews_per_book=3):
    """
    Generates simulated customer reviews and performs sentiment analysis.
    """
    sentences = [
        "This was a fantastic read!",
        "I really enjoyed this book.",
        "The plot was engaging and well-developed.",
        "I couldn't put it down!",
        "Highly recommend this book to others.",
        "It was okay, not my favorite.",
        "The story was a bit slow at times.",
        "I had trouble connecting with the characters.",
        "It wasn't as good as I expected.",
        "I wouldn't read it again.",
        "Terrible book, waste of time.",
        "The worst book ever.",
        "I hated it so much.",
        "Do not read this",
        "The story was boring and predictable.",
    ]

    sid = SentimentIntensityAnalyzer()  # Initialize sentiment analyzer
    df['simulated_reviews'] = ''
    df['compound_sentiment'] = 0.0  #Initialize to 0 to be able to aggregate later

    for index, row in df.iterrows():
        reviews = random.choices(sentences, k=num_reviews_per_book)
        df.at[index, 'simulated_reviews'] = reviews

        # Calculate aggregate sentiment
        total_sentiment = 0
        for review in reviews:
            scores = sid.polarity_scores(review)
            total_sentiment += scores['compound']
        df.at[index, 'compound_sentiment'] = total_sentiment / num_reviews_per_book #aggregate compound sentiment

def analyze_sentiment(df):
    """Analyzes the sentiment of simulated customer reviews."""
    print("\nSentiment Analysis:")
    print(f"Average compound sentiment: {df['compound_sentiment'].mean():.2f}")
    #Further analysis is possible, such as classifying sentiment, or counting positive vs negative reviews.

if __name__ == "__main__":
    base_url = "http://books.toscrape.com/catalogue/category/books/mystery_3/"  # Example: Mystery books

    num_pages = 2  # Adjust as needed, but start small!

    book_data = scrape_all_books(base_url, num_pages)

    if not book_data.empty:
        print("\nScraped Data:")
        print(book_data)

        analyze_pricing(book_data)
        analyze_ratings(book_data)

        # Generate and analyze simulated reviews:
        generate_simulated_reviews(book_data) #Added
        analyze_sentiment(book_data)

        # Further analysis and visualization can be added here.
    else:
        print("No book data scraped.")